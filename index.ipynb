{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### read train.csv and drop NaN\n",
    "dataframe = pd.read_csv('dataset/train.csv')\n",
    "temp = dataframe.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300000, 2)\n",
      "(300072, 9)\n",
      "(300000, 9)\n"
     ]
    }
   ],
   "source": [
    "# ### run test.csv\n",
    "# dataframe_2 = pd.read_csv('dataset/test.csv')\n",
    "\n",
    "# ### Run test on 300000 that both train and test set have.\n",
    "# # print(dataframe_2.shape)\n",
    "# df_unknown = dataframe.drop(temp.index)\n",
    "# # print(df_unknown.shape) # 300072\n",
    "# diff = df_unknown[['Id']].drop(np.array(dataframe_2['Id']))\n",
    "# df_2 = df_unknown[['Id']].drop(diff.index)\n",
    "# test_metadata = dataframe.iloc[np.array(df_2).reshape(300000,),:]\n",
    "# # print(test_metadata.shape) # 300000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \n",
    "             \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \n",
    "             \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \n",
    "             \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \n",
    "             \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\",\n",
    "             \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\",\n",
    "             \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\",\n",
    "             \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \n",
    "             \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \n",
    "             \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\",\n",
    "             \"now\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "### Preprocess train data\n",
    "reviews_df2 = temp.sample(n=200)\n",
    "reviews_df2['helpful%'] = np.round_(np.where(reviews_df2['HelpfulnessDenominator'] > 0,\n",
    "                                  reviews_df2['HelpfulnessNumerator'] / reviews_df2['HelpfulnessDenominator'], -1),1)\n",
    "\n",
    "reviews_df2['helpfulness_range'] = pd.cut(x=reviews_df2['helpful%'], bins=[-1, -0.1, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "                                         labels=['0', '1', '2', '3', '4', '5'], include_lowest=True)\n",
    "\n",
    "# for comments, get rid of punctuation and make them as all lowercase\n",
    "\n",
    "def text_process(reviewText):\n",
    "    nopunc = [i for i in reviewText if i not in string.punctuation]\n",
    "    nopunc = [nopunc.lower() for nopunc in nopunc]\n",
    "    nopunc_text = ''.join(nopunc)\n",
    "    return [i for i in nopunc_text.split() if i not in stopwords]\n",
    "text = []\n",
    "\n",
    "for i in range(reviews_df2['Summary'].shape[0]):\n",
    "    if(i%10000==0):\n",
    "        print(i)\n",
    "    text.append(text_process(np.array(reviews_df2['Summary'])[i]))\n",
    "reviews_df2['Summary'] = text\n",
    "\n",
    "text2 = []\n",
    "for i in range(reviews_df2['Text'].shape[0]):\n",
    "    text2.append(text_process(np.array(reviews_df2['Text'])[i]))\n",
    "reviews_df2['Text'] = text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### preprocessing test data\n",
    "# reviews_df = test_metadata\n",
    "\n",
    "\n",
    "# # create a parameter to check whether a score is helpful, if no one votes helpful at all, certain comment will\n",
    "# # return -1\n",
    "\n",
    "# reviews_df['helpful%'] = np.round_(np.where(reviews_df['HelpfulnessDenominator'] > 0,\n",
    "#                                   reviews_df['HelpfulnessNumerator'] / reviews_df['HelpfulnessDenominator'], -1),1)\n",
    "\n",
    "# reviews_df['helpfulness_range'] = pd.cut(x=reviews_df['helpful%'], bins=[-1, -0.1, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "#                                          labels=['0', '1', '2', '3', '4', '5'], include_lowest=True)\n",
    "\n",
    "# # for comments, get rid of punctuation and make them as all lowercase\n",
    "\n",
    "# def text_process(reviewText):\n",
    "#     nopunc = [i for i in reviewText if i not in string.punctuation]\n",
    "#     nopunc = [nopunc.lower() for nopunc in nopunc]\n",
    "#     nopunc_text = ''.join(nopunc)\n",
    "#     return [i for i in nopunc_text.split() if i not in stopwords]\n",
    "# text = []\n",
    "# for i in range(reviews_df['Summary'].shape[0]):\n",
    "#     if i%10000==0:\n",
    "#         print(i)\n",
    "#     text.append(text_process(np.array(reviews_df['Summary'])[i]))\n",
    "# reviews_df['Summary'] = text\n",
    "\n",
    "# text2 = []\n",
    "# for i in range(reviews_df['Text'].shape[0]):\n",
    "#     text2.append(text_process(np.array(reviews_df['Text'])[i]))\n",
    "# reviews_df['Text'] = text2\n",
    "# print(reviews_df[['Summary','Text']])\n",
    "# print(reviews_df[['HelpfulnessNumerator','HelpfulnessDenominator','helpful%','helpfulness_range']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4781,) (2005,)\n",
      "0\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "### count score of positive and negative words\n",
    "# if positive word is found +1, negative word is found -1\n",
    "with open('negative-words.txt', 'r',encoding = \"ISO-8859-1\") as f:\n",
    "    x = f.readlines()\n",
    "neg_words = np.char.strip(np.array(x),'\\n')\n",
    "\n",
    "with open('positive-words.txt', 'r',encoding = \"ISO-8859-1\") as f:\n",
    "    y = f.readlines()\n",
    "pos_words = np.char.strip(np.array(y),'\\n')\n",
    "print(neg_words.shape,pos_words.shape)\n",
    "\n",
    "score_list = []\n",
    "score = 0\n",
    "# print(reviews_df['Summary'])\n",
    "for j in range(reviews_df2['Text'].shape[0]):\n",
    "    list_words = np.array(reviews_df2['Text'])[j]\n",
    "    for k in range(len(list_words)):\n",
    "        if list_words[k] in pos_words:\n",
    "            score = score + 1\n",
    "        elif list_words[k] in neg_words:\n",
    "            score = score - 1\n",
    "    score_list.append(score)\n",
    "    score = 0\n",
    "score_list_scaled = preprocessing.scale(score_list)\n",
    "reviews_df2['Text_score'] = np.round(score_list_scaled,2)\n",
    "\n",
    "score = 0\n",
    "score_list = []\n",
    "for j in range(reviews_df2['Summary'].shape[0]):\n",
    "    if(j%10000==0):\n",
    "        print(j)\n",
    "    list_words = np.array(reviews_df2['Summary'])[j]\n",
    "    for k in range(len(list_words)):\n",
    "        if list_words[k] in pos_words:\n",
    "            score = score + 1\n",
    "        elif list_words[k] in neg_words:\n",
    "            score = score - 1\n",
    "    score_list.append(score)\n",
    "    score = 0\n",
    "score_list_scaled = preprocessing.scale(score_list)\n",
    "reviews_df2['Summary_score'] = np.round(score_list_scaled,2)\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Weight text_score and summary_score\n",
    "alpha = 0\n",
    "final_score = (reviews_df2['Summary_score']*alpha + reviews_df2['Text_score'])/(1+alpha)\n",
    "reviews_df2['Final_score'] = np.round(final_score,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 14)\n"
     ]
    }
   ],
   "source": [
    "# split train and label\n",
    "tr = reviews_df2[['Id','ProductId','UserId',\n",
    "         'HelpfulnessNumerator','HelpfulnessDenominator']]\n",
    "# change string to integer\n",
    "col = reviews_df2['ProductId']\n",
    "reviews_df2[['Id','ProductId','UserId',\n",
    "         'HelpfulnessNumerator','HelpfulnessDenominator']] = tr.apply(lambda col: pd.factorize(col)[0])\n",
    "reviews_df2['Time'] = preprocessing.scale(reviews_df2['Time'])\n",
    "print(reviews_df2.shape)\n",
    "\n",
    "# tr = reviews_df2[['Id','ProductId','UserId',\n",
    "#          'HelpfulnessNumerator','HelpfulnessDenominator']]\n",
    "# # change string to integer\n",
    "# col = reviews_df2['ProductId']\n",
    "# reviews_df2[['Id','ProductId','UserId',\n",
    "#          'HelpfulnessNumerator','HelpfulnessDenominator']] = tr.apply(lambda col: pd.factorize(col)[0])\n",
    "# reviews_df2['Time'] = preprocessing.scale(reviews_df2['Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_ = test_metadata[['Id','ProductId','UserId','HelpfulnessNumerator','HelpfulnessDenominator','Time']]\n",
    "# col_ = test_['ProductId']\n",
    "# test = test_.apply(lambda col_: pd.factorize(col_)[0]).sample(n=5000)\n",
    "# df2 = temp.drop(df.index)\n",
    "# df2 = df2.sample(n=20000)\n",
    "# # split train and label\n",
    "# tr2 = df2[['Id','ProductId','UserId','HelpfulnessNumerator','HelpfulnessDenominator']] #,'Time'\n",
    "# # change string to integer\n",
    "# col2 = df2['ProductId']\n",
    "# x_test = tr2.apply(lambda col2: pd.factorize(col2)[0])\n",
    "# print(x_test)\n",
    "# y_test = df2[['Score']]\n",
    "# print(reviews_df)\n",
    "x = reviews_df2[['ProductId','UserId','HelpfulnessDenominator','HelpfulnessNumerator']]\n",
    "y = reviews_df2['Score']\n",
    "# x_test = reviews_df[['ProductId','UserId','HelpfulnessDenominator','HelpfulnessNumerator']]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)\n",
    "# print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 4)\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60.62"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_ypred = []\n",
    "# Logistic Regression\n",
    "\n",
    "logreg = LogisticRegression(solver='lbfgs',multi_class='multinomial')\n",
    "logreg.fit(x_train,y_train)\n",
    "Y_pred = logreg.predict(x_test)\n",
    "count = 0\n",
    "for i in range(Y_pred.shape[0]):\n",
    "    if (Y_pred[i,]-np.array(y_test).reshape(x_test.shape[0],)[i,] == 0):\n",
    "        count +=1\n",
    "print(count/Y_pred.shape[0])\n",
    "acc_log = round(logreg.score(x_train, y_train) * 100, 2)\n",
    "acc_log\n",
    "# list_ypred.append(count/Y_pred.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.55\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Support Vector Machines\n",
    "svc = LinearSVC()\n",
    "svc.fit(x_train, y_train)\n",
    "Y_pred = svc.predict(x_test)\n",
    "count = 0\n",
    "for i in range(Y_pred.shape[0]):\n",
    "    if (Y_pred[i,]-np.array(y_test).reshape(x_test.shape[0],)[i,] == 0):\n",
    "        count +=1\n",
    "print(count/Y_pred.shape[0])\n",
    "acc_svc = round(svc.score(x_train, y_train) * 100, 2)\n",
    "acc_svc\n",
    "# print(Y_pred)\n",
    "# # list_ypred.append(count/Y_pred.shape[0])\n",
    "# reviews_df['Score'] = Y_pred\n",
    "# reviews_df = reviews_df[['Id','Score']]\n",
    "# print(reviews_df.shape)\n",
    "# reviews_df.to_csv('dataset/output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(y_test, Y_pred[:60,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "64.38"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# K Nearest Neighbors Classifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "knn.fit(x_train, y_train)\n",
    "Y_pred = knn.predict(x_test)\n",
    "count = 0\n",
    "for i in range(Y_pred.shape[0]):\n",
    "    if (Y_pred[i,]-np.array(y_test).reshape(x_test.shape[0],)[i,] == 0):\n",
    "        count +=1\n",
    "print(count/Y_pred.shape[0])\n",
    "acc_knn = round(knn.score(x_train, y_train) * 100, 2)\n",
    "acc_knn\n",
    "# list_ypred.append(count/Y_pred.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.55\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "59.38"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "sgd = SGDClassifier()\n",
    "sgd.fit(x_train, y_train)\n",
    "Y_pred = sgd.predict(x_test)\n",
    "count = 0\n",
    "for i in range(Y_pred.shape[0]):\n",
    "    if (Y_pred[i,]-np.array(y_test).reshape(x_test.shape[0],)[i,] == 0):\n",
    "        count +=1\n",
    "print(count/Y_pred.shape[0])\n",
    "acc_sgd = round(sgd.score(x_train, y_train) * 100, 2)\n",
    "acc_sgd\n",
    "# list_ypred.append(count/Y_pred.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decision Tree\n",
    "\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(x_train, y_train)\n",
    "Y_pred = decision_tree.predict(x_test)\n",
    "count = 0\n",
    "for i in range(Y_pred.shape[0]):\n",
    "    if (Y_pred[i,]-np.array(y_test).reshape(x_test.shape[0],)[i,] == 0):\n",
    "        count +=1\n",
    "print(count/Y_pred.shape[0])\n",
    "acc_decision_tree = round(decision_tree.score(x_train, y_train) * 100, 2)\n",
    "acc_decision_tree\n",
    "# list_ypred.append(count/Y_pred.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "random_forest.fit(x_train, y_train)\n",
    "Y_pred = random_forest.predict(x_test)\n",
    "# print(Y_pred)\n",
    "# print(np.array(y_test).reshape(x_test.shape[0],))\n",
    "count = 0\n",
    "for i in range(Y_pred.shape[0]):\n",
    "    if (Y_pred[i,]-np.array(y_test).reshape(x_test.shape[0],)[i,] == 0):\n",
    "        count +=1\n",
    "print(count/Y_pred.shape[0])\n",
    "acc_random_forest = round(random_forest.score(x_train, y_train) * 100, 2)\n",
    "acc_random_forest\n",
    "# list_ypred.append(count/Y_pred.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-9252429b4630>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', 'Random Forest', \n\u001b[1;32m      4\u001b[0m               'Stochastic Gradient Decent','Decision Tree'],\n\u001b[0;32m----> 5\u001b[0;31m     'Score': list_ypred})\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    346\u001b[0m                                  dtype=dtype, copy=copy)\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_init_dict\u001b[0;34m(self, data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_arrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_arrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[1;32m   7354\u001b[0m     \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7356\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7358\u001b[0m     \u001b[0;31m# don't force copy because getting jammed in an ndarray anyway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mextract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   7400\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7401\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7402\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'arrays must all be same length'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7404\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: arrays must all be same length"
     ]
    }
   ],
   "source": [
    "print(list_ypred)\n",
    "models = pd.DataFrame({\n",
    "    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', 'Random Forest', \n",
    "              'Stochastic Gradient Decent','Decision Tree'],\n",
    "    'Score': list_ypred})\n",
    "models.sort_values(by='Score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
